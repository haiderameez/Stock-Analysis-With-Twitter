{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification failed: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rameez\n",
      "[nltk_data]     Haider\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Rameez\n",
      "[nltk_data]     Haider\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Rameez\n",
      "[nltk_data]     Haider\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Rameez\n",
      "[nltk_data]     Haider\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rameez Haider\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk_data_dir = os.path.join(os.path.expanduser('~'), 'nltk_data')\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "\n",
    "required_resources = ['punkt', 'stopwords', 'wordnet', 'vader_lexicon', 'averaged_perceptron_tagger']\n",
    "for resource in required_resources:\n",
    "    nltk.download(resource, download_dir=nltk_data_dir)\n",
    "\n",
    "try:\n",
    "    for resource in required_resources:\n",
    "        nltk.data.find(resource)\n",
    "    print(\"All resources are properly downloaded and verified.\")\n",
    "except Exception as e:\n",
    "    print(f\"Verification failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data[['Timestamp', 'Content', 'Comments', 'Retweets', 'Likes', 'Analytics']]  \n",
    "    data.dropna(subset=['Content'], inplace=True) \n",
    "    data = convert_columns_to_numbers(data, ['Comments', 'Retweets', 'Likes', 'Analytics'])  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Abbreviated Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_columns_to_numbers(df, columns):\n",
    "    def convert_abbreviated_number(value):\n",
    "        if isinstance(value, str):\n",
    "            value = value.upper()\n",
    "            if 'K' in value:\n",
    "                return int(float(value.replace('K', '')) * 1_000)\n",
    "            elif 'M' in value:\n",
    "                return int(float(value.replace('M', '')) * 1_000_000)\n",
    "            elif 'B' in value:\n",
    "                return int(float(value.replace('B', '')) * 1_000_000_000)\n",
    "        try:\n",
    "            return int(value)\n",
    "        except ValueError:\n",
    "            return value  \n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].apply(convert_abbreviated_number)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r\"http\\S+|@\\S+|#\\S+|[^A-Za-z\\s]\", \"\", tweet) \n",
    "    tweet = tweet.lower() \n",
    "    tokens = word_tokenize(tweet) t\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')] \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mentions(content):\n",
    "    return re.findall(r'@\\w+', content)\n",
    "\n",
    "def extract_hashtags(content):\n",
    "    return re.findall(r'#\\w+', content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Top Terms and Counting Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_terms(data, column, n_terms=10):\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features=n_terms)\n",
    "    term_matrix = vectorizer.fit_transform(data[column])\n",
    "    return vectorizer.get_feature_names_out()\n",
    "\n",
    "def count_mentions(tweet, keywords):\n",
    "    return sum(1 for word in tweet.split() if word in keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_sentiment_analysis(data):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    data['sentiment_textblob'] = data['Content'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    data['sentiment_vader'] = data['Content'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to 'processed_twitter_data.csv'\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    file_path = \"path\"  \n",
    "    data = load_and_clean_csv(file_path)\n",
    "\n",
    "    data['cleaned_content'] = data['Content'].apply(preprocess_tweet)\n",
    "\n",
    "    data['Mentions_List'] = data['Content'].apply(extract_mentions)\n",
    "    data['Hashtags_List'] = data['Content'].apply(extract_hashtags)\n",
    "\n",
    "    top_mentions = data['Mentions_List'].explode().value_counts().index[:10].tolist()\n",
    "    top_hashtags = data['Hashtags_List'].explode().value_counts().index[:10].tolist()\n",
    "    top_terms = get_top_terms(data, 'cleaned_content', n_terms=10)\n",
    "\n",
    "    keywords = top_mentions + top_hashtags + list(top_terms)\n",
    "\n",
    "    data['mention_count'] = data['cleaned_content'].apply(lambda x: count_mentions(x, keywords))\n",
    "\n",
    "    data = perform_sentiment_analysis(data)\n",
    "\n",
    "    data.to_csv(\"processed_twitter_data.csv\", index=False)\n",
    "    print(\"Processed data saved to 'processed_twitter_data.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
